{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wasserstein super-resolution emulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T11:54:41.053245Z",
     "start_time": "2018-06-13T11:54:38.458900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/soft/python/3.7.3/lib/python3.7/site-packages/pandas/compat/__init__.py:84: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/soft/python/3.7.3/lib/python3.7/site-packages/pandas/compat/__init__.py:84: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load low and high-resolution density fields, and the initial conditions for the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_resolution = 512\n",
    "lr_resolution = 256\n",
    "\n",
    "hr_final = np.load(\"high_res/df_0_z=0.npy\")\n",
    "hr_final = hr_final.astype(np.float32).reshape((hr_resolution, hr_resolution, hr_resolution))\n",
    "\n",
    "hr_initial = np.load(\"high_res/df_0_z=127.npy\")\n",
    "hr_initial = hr_initial.astype(np.float32).reshape((hr_resolution, hr_resolution, hr_resolution))\n",
    "\n",
    "lr_final = np.load(\"low_res/df_0_z=0.npy\")\n",
    "lr_final = lr_final.astype(np.float32).reshape((lr_resolution, lr_resolution, lr_resolution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize final density fields such that mean density guaranteed to be unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_final /= hr_final.sum()/hr_resolution**3\n",
    "lr_final /= lr_final.sum()/lr_resolution**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the TensorFlow graph\n",
    "\n",
    "### Data augmentation\n",
    "We are going to perform data augmentation on the fly to save time. In particular, we will perform right angle rotations, with a total of 24 possibilities, to randomly selected 3D slices of training set. Moreover, we also apply a random flip along the three axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotations(y):\n",
    "\n",
    "    x = y[0]\n",
    "    rand = y[1]\n",
    "    inv = y[2]\n",
    "    x = tf.case(\n",
    "        {tf.equal(rand, 1): lambda: x[:, ::-1, ::-1, :, :],\n",
    "         tf.equal(rand, 2): lambda: x[:, ::-1, :, ::-1, :],\n",
    "         tf.equal(rand, 3): lambda: x[:, :, ::-1, ::-1, :],\n",
    "         tf.equal(rand, 4): lambda: tf.transpose(x, (0, 2, 1, 3, 4))[:, ::-1, :, :, :],\n",
    "         tf.equal(rand, 5): lambda: tf.transpose(x, (0, 2, 1, 3, 4))[:, ::-1, :, ::-1, :],\n",
    "         tf.equal(rand, 6): lambda: tf.transpose(x, (0, 2, 1, 3, 4))[:, :, ::-1, :, :],\n",
    "         tf.equal(rand, 7): lambda: tf.transpose(x, (0, 2, 1, 3, 4))[:, :, ::-1, ::-1, :],\n",
    "         tf.equal(rand, 8): lambda: tf.transpose(x, (0, 3, 2, 1, 4))[:, ::-1, :, :, :],\n",
    "         tf.equal(rand, 9): lambda: tf.transpose(x, (0, 3, 2, 1, 4))[:, ::-1, ::-1, :, :],\n",
    "         tf.equal(rand, 10): lambda: tf.transpose(x, (0, 3, 2, 1, 4))[:, :, :, ::-1, :],\n",
    "         tf.equal(rand, 11): lambda: tf.transpose(x, (0, 3, 2, 1, 4))[:, :, ::-1, ::-1, :],\n",
    "         tf.equal(rand, 12): lambda: tf.transpose(x, (0, 1, 3, 2, 4))[:, :, ::-1, :, :],\n",
    "         tf.equal(rand, 13): lambda: tf.transpose(x, (0, 1, 3, 2, 4))[:, ::-1, ::-1, :, :],\n",
    "         tf.equal(rand, 14): lambda: tf.transpose(x, (0, 1, 3, 2, 4))[:, :, :, ::-1, :],\n",
    "         tf.equal(rand, 15): lambda: tf.transpose(x, (0, 1, 3, 2, 4))[:, ::-1, :, ::-1, :],\n",
    "         tf.equal(rand, 16): lambda: tf.transpose(x, (0, 2, 3, 1, 4))[:, ::-1, ::-1, :, :],\n",
    "         tf.equal(rand, 17): lambda: tf.transpose(x, (0, 2, 3, 1, 4))[:, :, ::-1, ::-1, :],\n",
    "         tf.equal(rand, 18): lambda: tf.transpose(x, (0, 2, 3, 1, 4))[:, ::-1, :, ::-1, :],\n",
    "         tf.equal(rand, 19): lambda: tf.transpose(x, (0, 2, 3, 1, 4))[:, ::-1, ::-1, ::-1, :], \n",
    "         tf.equal(rand, 20): lambda: tf.transpose(x, (0, 3, 1, 2, 4))[:, ::-1, ::-1, :, :],\n",
    "         tf.equal(rand, 21): lambda: tf.transpose(x, (0, 3, 1, 2, 4))[:, ::-1, :, ::-1, :],\n",
    "         tf.equal(rand, 22): lambda: tf.transpose(x, (0, 3, 1, 2, 4))[:, :, ::-1, ::-1, :],\n",
    "         tf.equal(rand, 23): lambda: tf.transpose(x, (0, 3, 1, 2, 4))[:, ::-1, ::-1, ::-1, :],\n",
    "         tf.equal(rand, 24): lambda: x}, default = lambda: x, exclusive = True)\n",
    "    \n",
    "    return tf.case({tf.equal(inv, 1): lambda: x[:, ::-1, ::-1, ::-1, :]}, default = lambda: x, exclusive = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the critic network\n",
    "The critic encodes a series of four convolutional layers, with a gradual reduction in their kernel sizes from 7x7x7 to 1x1x1, activated with $\\texttt{leaky ReLU}$. The critic takes as input the high-resolution initial conditions and real density field. The output of the final convolutional layer is flattened and fed into a fully connected layer with linear activation.\n",
    "Essentially, the critic reduces the input real and generated 3D high-resolution density fields to a compact representation whose difference is an approximation to the Wasserstein distance, conditional on their respective initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T11:54:42.318191Z",
     "start_time": "2018-06-13T11:54:42.224615Z"
    }
   },
   "outputs": [],
   "source": [
    "def W(density, IC, pad_hr, slice_size_hr, reference=False):\n",
    "\n",
    "    IC = tf.slice(IC, [0, pad_hr, pad_hr, pad_hr, 0], [-1, slice_size_hr-2*pad_hr, slice_size_hr-2*pad_hr, slice_size_hr-2*pad_hr, -1])\n",
    "    print(\"IC\")\n",
    "    print(IC)\n",
    "    \n",
    "    if reference:\n",
    "        density = tf.slice(density, [0, pad_hr, pad_hr, pad_hr, 0], [-1, slice_size_hr-2*pad_hr, slice_size_hr-2*pad_hr, slice_size_hr-2*pad_hr, -1])\n",
    "        print(\"density\")\n",
    "        print(density)\n",
    "    \n",
    "    data = tf.concat((density, IC), axis=4)\n",
    "    print(\"data\")\n",
    "    print(data)\n",
    "\n",
    "    w1 = tf.get_variable(\"W_w1\", \n",
    "                         (7, 7, 7, 2, 8), \n",
    "                         dtype=tf.float32, \n",
    "                         initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    b1 = tf.get_variable(\"W_b1\", \n",
    "                         (8), \n",
    "                         dtype=tf.float32, \n",
    "                         initializer=tf.constant_initializer(0.1))\n",
    "    x1 = tf.nn.leaky_relu(tf.nn.conv3d(data, \n",
    "                                       w1, \n",
    "                                       strides=[1, 2, 2, 2, 1], \n",
    "                                       padding='VALID') + b1, 0.1)\n",
    "    print(x1)\n",
    "\n",
    "    w2 = tf.get_variable(\"W_w2\",\n",
    "                         (5, 5, 5, 8, 16),\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    b2 = tf.get_variable(\"W_b2\",\n",
    "                         (16),\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.constant_initializer(0.1))\n",
    "    x2 = tf.nn.leaky_relu(tf.nn.conv3d(x1, \n",
    "                                       w2, \n",
    "                                       strides=[1, 1, 1, 1, 1], \n",
    "                                       padding='VALID') + b2, 0.1)\n",
    "    print(x2)    \n",
    "    w3 = tf.get_variable(\"W_w3\",\n",
    "                         (3, 3, 3, 16, 32),\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    b3 = tf.get_variable(\"W_b3\",\n",
    "                         (32),\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.constant_initializer(0.1))\n",
    "    x3 = tf.nn.leaky_relu(tf.nn.conv3d(x2, \n",
    "                                       w3, \n",
    "                                       strides=[1, 2, 2, 2, 1], \n",
    "                                       padding='VALID') + b3, 0.1)\n",
    "    \n",
    "    w4 = tf.get_variable(\"W_w4\",\n",
    "                         (1, 1, 1, 32, 64),\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    b4 = tf.get_variable(\"W_b4\",\n",
    "                         (64),\n",
    "                         dtype=tf.float32,\n",
    "                         initializer=tf.constant_initializer(0.1))\n",
    "    x4 = tf.nn.leaky_relu(tf.nn.conv3d(x3, \n",
    "                                       w4, \n",
    "                                       strides=[1, 1, 1, 1, 1], \n",
    "                                       padding='VALID') + b4, 0.1)    \n",
    "        \n",
    "    x5 = tf.reshape(x4, (-1, np.product(x4.get_shape().as_list()[1:])))\n",
    "    w5 = tf.get_variable(\"W_w5\", \n",
    "                         (x5.get_shape().as_list()[-1], 1), \n",
    "                         dtype=tf.float32, \n",
    "                         initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    b5 = tf.get_variable(\"W_b5\", \n",
    "                         (1), \n",
    "                         dtype=tf.float32, \n",
    "                         initializer=tf.constant_initializer(0.1))\n",
    "    x_out = tf.matmul(x5, w5) + b5\n",
    "    \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual inception module\n",
    "We employ Inception blocks, encoding residual connections in a modified variant of the originally proposed Inception module, in the architecture of our super-resolution emulator, as illustrated below. The largest convolutional kernel in the Inception module is $7\\times7\\times7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T11:54:42.436796Z",
     "start_time": "2018-06-13T11:54:42.321337Z"
    }
   },
   "outputs": [],
   "source": [
    "def inception(x, filters_1x1x1_7x7x7, filters_7x7x7, filters_1x1x1_5x5x5, filters_5x5x5, filters_1x1x1_3x3x3, filters_3x3x3, filters_1x1x1, name):\n",
    "    input_filters = x.get_shape().as_list()[-1]\n",
    "\n",
    "    w_1x1x1_7x7x7 = tf.get_variable(name + \"_w_1x1x1_7x7x7\", (1, 1, 1, input_filters, filters_1x1x1_7x7x7), dtype = tf.float32, initializer = tf.random_normal_initializer(0, 0.1))\n",
    "    b_1x1x1_7x7x7 = tf.get_variable(name + \"_b_1x1x1_7x7x7\", (filters_1x1x1_7x7x7), dtype = tf.float32, initializer = tf.constant_initializer(0.1))\n",
    "    x_1x1x1_7x7x7 = tf.nn.conv3d(x, w_1x1x1_7x7x7, strides = [1, 1, 1, 1, 1], padding = 'VALID') + b_1x1x1_7x7x7\n",
    "    w_7x7x7 = tf.get_variable(name + \"_w_7x7x7\", (7, 7, 7, filters_1x1x1_7x7x7, filters_7x7x7), dtype = tf.float32, initializer = tf.random_normal_initializer(0, 0.1))\n",
    "    b_7x7x7 = tf.get_variable(name + \"_b_7x7x7\", (filters_7x7x7), dtype = tf.float32, initializer = tf.constant_initializer(0.1))\n",
    "    x_7x7x7 = tf.nn.conv3d(x_1x1x1_7x7x7, w_7x7x7, strides = [1, 1, 1, 1, 1], padding = 'VALID') + b_7x7x7    \n",
    "    \n",
    "    w_1x1x1_5x5x5 = tf.get_variable(name + \"_w_1x1x1_5x5x5\", (1, 1, 1, input_filters, filters_1x1x1_5x5x5), dtype = tf.float32, initializer = tf.random_normal_initializer(0, 0.1))\n",
    "    b_1x1x1_5x5x5 = tf.get_variable(name + \"_b_1x1x1_5x5x5\", (filters_1x1x1_5x5x5), dtype = tf.float32, initializer = tf.constant_initializer(0.1))\n",
    "    x_1x1x1_5x5x5 = tf.nn.conv3d(x, w_1x1x1_5x5x5, strides = [1, 1, 1, 1, 1], padding = 'VALID') + b_1x1x1_5x5x5\n",
    "    w_5x5x5 = tf.get_variable(name + \"_w_5x5x5\", (5, 5, 5, filters_1x1x1_5x5x5, filters_5x5x5), dtype = tf.float32, initializer = tf.random_normal_initializer(0, 0.1))\n",
    "    b_5x5x5 = tf.get_variable(name + \"_b_5x5x5\", (filters_5x5x5), dtype = tf.float32, initializer = tf.constant_initializer(0.1))\n",
    "    x_5x5x5 = tf.nn.conv3d(x_1x1x1_5x5x5, w_5x5x5, strides = [1, 1, 1, 1, 1], padding = 'VALID') + b_5x5x5\n",
    "    x_5x5x5 = tf.slice(x_5x5x5, [0, 1, 1, 1, 0], [-1, x_7x7x7.get_shape().as_list()[1], x_7x7x7.get_shape().as_list()[2], x_7x7x7.get_shape().as_list()[3], -1])\n",
    "    \n",
    "    w_1x1x1_3x3x3 = tf.get_variable(name + \"_w_1x1x1_3x3x3\", (1, 1, 1, input_filters, filters_1x1x1_3x3x3), dtype = tf.float32, initializer = tf.random_normal_initializer(0, 0.1))\n",
    "    b_1x1x1_3x3x3 = tf.get_variable(name + \"_b_1x1x1_3x3x3\", (filters_1x1x1_3x3x3), dtype = tf.float32, initializer = tf.constant_initializer(0.1))\n",
    "    x_1x1x1_3x3x3 = tf.nn.conv3d(x, w_1x1x1_3x3x3, strides = [1, 1, 1, 1, 1], padding = 'VALID') + b_1x1x1_3x3x3\n",
    "    w_3x3x3 = tf.get_variable(name + \"_w_3x3x3\", (3, 3, 3, filters_1x1x1_3x3x3, filters_3x3x3), dtype = tf.float32, initializer = tf.random_normal_initializer(0, 0.1))\n",
    "    b_3x3x3 = tf.get_variable(name + \"_b_3x3x3\", (filters_3x3x3), dtype = tf.float32, initializer = tf.constant_initializer(0.1))\n",
    "    x_3x3x3 = tf.nn.conv3d(x_1x1x1_3x3x3, w_3x3x3, strides = [1, 1, 1, 1, 1], padding = 'VALID') + b_3x3x3\n",
    "    x_3x3x3 = tf.slice(x_3x3x3, [0, 2, 2, 2, 0], [-1, x_7x7x7.get_shape().as_list()[1], x_7x7x7.get_shape().as_list()[2], x_7x7x7.get_shape().as_list()[3], -1])\n",
    "    \n",
    "    w_1x1x1 = tf.get_variable(name + \"_w_1x1x1\", (1, 1, 1, input_filters, filters_1x1x1), dtype = tf.float32, initializer = tf.random_normal_initializer(0, 0.1))\n",
    "    b_1x1x1 = tf.get_variable(name + \"_b_1x1x1\", (filters_1x1x1), dtype = tf.float32, initializer = tf.constant_initializer(0.1))\n",
    "    x_1x1x1 = tf.nn.conv3d(x, w_1x1x1, strides = [1, 1, 1, 1, 1], padding = 'SAME') + b_1x1x1\n",
    "    x_1x1x1 = tf.slice(x_1x1x1, [0, 3, 3, 3, 0], [-1, x_7x7x7.get_shape().as_list()[1], x_7x7x7.get_shape().as_list()[2], x_7x7x7.get_shape().as_list()[3], -1])\n",
    "\n",
    "    x_out = tf.concat((x_7x7x7, x_5x5x5, x_3x3x3, x_1x1x1), axis = 4)\n",
    "    output_filters = x_out.get_shape().as_list()[-1]\n",
    "    x_out_ = tf.slice(tf.transpose(tf.stack([x[:, :, :, :, 0] for i in range(output_filters)]), perm = [1, 2, 3, 4, 0]), [0, 2, 2, 2, 0], [-1, x_5x5x5.get_shape().as_list()[1], x_5x5x5.get_shape().as_list()[2], x_5x5x5.get_shape().as_list()[3], -1])\n",
    "    \n",
    "    return tf.add(x_out, x_out_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1x1x1 convolution\n",
    "\n",
    "To be used as the output layer in the super-resolution emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_1x1x1_convolutions(x_in, filters_1x1x1, name):\n",
    "    input_filters = x_in.get_shape().as_list()[-1]\n",
    "        \n",
    "    w_1x1x1 = tf.get_variable(name + \"_w_1x1x1\",\n",
    "                              (1, 1, 1, input_filters, filters_1x1x1),\n",
    "                              dtype=tf.float32,\n",
    "                              initializer=tf.random_normal_initializer(0, 0.1))\n",
    "    b_1x1x1 = tf.get_variable(name + \"_b_1x1x1\",\n",
    "                              (filters_1x1x1),\n",
    "                              dtype=tf.float32,\n",
    "                              initializer=tf.constant_initializer(0.1))\n",
    "    x_1x1x1 = tf.nn.conv3d(x_in, w_1x1x1, strides = [1, 1, 1, 1, 1], padding='SAME') + b_1x1x1\n",
    "    \n",
    "    return x_1x1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the super-resolution emulator\n",
    "Our emulator takes as input a low-resolution density field cube and augments the features to yield the corresponding high-resolution density field.\n",
    "\n",
    "The residual Inception blocks propagates the information from fairly local regions of the dark matter field, and since we use residual connections in the Inception blocks, we combine structure from distant patches, whilst still retaining a close relation to the density field itself. The subsequent strided 1x1x1 convolution, with no residual connection, leads to the desired larger array size. Since there is an enormous complexity in the low-res density field, we use many filters in each layer to learn the wide variety of possible features. \n",
    "\n",
    "The architecture of the emulator is driven by simple physical principles. It is designed to perform a physical mapping that encodes some fundamental symmetries (rotational and translational invariance) and as such, the cosmological principle, and the maximum extent of the receptive field (i.e. size of the largest convolutional kernel in the Inception module) is motivated by causal transport arguments to ensure that the non-local information is captured by the network. The non-linearity involved in this super-resolution procedure is provided by the $\\texttt{leaky ReLU}$ activation with a leaky parameter of $\\alpha=0.1$. To ensure the positivity of the final high-resolution field, we use $\\texttt{ReLU}$ at the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G(δ_lr, δ_ic, num_convs, slice_size_hr=40, n_filters=6):\n",
    "    \n",
    "    print(δ_lr)\n",
    "    δ_lr = tf.keras.layers.UpSampling3D(size=(2, 2, 2), data_format=\"channels_last\")(δ_lr)\n",
    "    print(δ_lr)\n",
    "    \n",
    "    x = tf.nn.leaky_relu(inception(δ_lr, n_filters, n_filters, n_filters, n_filters, n_filters, n_filters, n_filters, 'layer_1'), 0.1)\n",
    "    print(x)\n",
    "\n",
    "    y = tf.nn.leaky_relu(inception(δ_ic, n_filters, n_filters, n_filters, n_filters, n_filters, n_filters, n_filters, 'layer_3'), 0.1)\n",
    "    print(y)\n",
    "        \n",
    "    x_out = tf.concat((x, y), axis=4)\n",
    "    print(x_out)\n",
    "    x_out = tf.nn.relu(inception(x_out, n_filters, n_filters, n_filters, n_filters, n_filters, n_filters, n_filters, 'layer_5'))\n",
    "    print(x_out)\n",
    "    \n",
    "    x_out = kernel_1x1x1_convolutions(x_out, 1, 'layer_out')\n",
    "    print(x_out)\n",
    "    \n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training methodology\n",
    "\n",
    "We choose the input to the emulator to be conveniently larger to eliminate the need for padding. We therefore compute the corresponding sizes of the tensors, which depends on the number of convolutional layers and the desired size of the prediction. If the network has two Inception modules, where the largest convolutional kernel is 7x7x7, then our input must be larger by $2\\times(7-1) = 12$ voxels on each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_convs = 2\n",
    "pad = 0\n",
    "input_patch = 20\n",
    "slice_size_lr = int(input_patch + 2*pad)\n",
    "slice_size_hr = input_patch*2\n",
    "\n",
    "pad_hr = int((num_convs * 6)/2)\n",
    "pad_lr = int(pad_hr/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we load both the entire low- and high-resolution density fields and initial conditions from the simulation into the TensorFlow graph and select by index sub-volume elements of size $40^3$ and $20^3$, respectively, which massively reduces computation time compared with passing the 3D slices of data at each weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "δ_lr_init = tf.placeholder(dtype=tf.float32,\n",
    "                        shape=(lr_resolution, lr_resolution, lr_resolution),\n",
    "                        name=\"initialise_low_res_density\")\n",
    "δ_lr = tf.Variable(δ_lr_init, trainable=False, name=\"low_res_density\")\n",
    "δ_lr_assign = tf.assign(δ_lr, δ_lr_init)\n",
    "\n",
    "δ_ic_init = tf.placeholder(dtype=tf.float32,\n",
    "                        shape=(hr_resolution, hr_resolution, hr_resolution),\n",
    "                        name=\"initialise_high_res_IC\")\n",
    "δ_ic = tf.Variable(δ_ic_init, trainable=False, name=\"high_res_IC\")\n",
    "δ_ic_assign = tf.assign(δ_ic, δ_ic_init)\n",
    "\n",
    "δ_hr_init = tf.placeholder(dtype=tf.float32,\n",
    "                        shape=(hr_resolution, hr_resolution, hr_resolution),\n",
    "                        name=\"initialise_high_res_density\")\n",
    "δ_hr = tf.Variable(δ_hr_init, trainable=False, name=\"high_res_density\")\n",
    "δ_hr_assign = tf.assign(δ_hr, δ_hr_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to feed into the network a random number for the rotation of the box, a set of x, y and z indices to grab the first element of the slice from the field. We need two sets of indices, for the low- and high-resolution slices, respectively. We also need a random number $\\epsilon=[0, 1]$ for the probability of the density field during the gradient penalty, and finally a number for the strength of the coupling of the gradient penalty. We will start with an initial batch size `m` of unity but will progressively double the batch size for every 100k weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_indices(ind, x_size, y_size, z_size):\n",
    "    size = x_size*y_size*z_size\n",
    "    all_indices = np.zeros((size*ind.shape[0], 3)).astype(np.int32)\n",
    "    \n",
    "    for i in range(ind.shape[0]):\n",
    "        counter = 0\n",
    "        for kk in range(z_size):\n",
    "            for jj in range(y_size):\n",
    "                for ii in range(x_size):\n",
    "                    all_indices[counter + i*size] = [ind[i, 0] + ii, ind[i, 1] + jj, ind[i, 2] + kk]\n",
    "                    counter += 1\n",
    "\n",
    "    return all_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation = tf.placeholder(dtype=tf.int32, shape=(None,), name=\"rotation\")\n",
    "inv = tf.placeholder(dtype=tf.int32, shape=(None,), name=\"inversion\")\n",
    "\n",
    "process_indices_lr = tf.expand_dims(tf.Variable(process_indices(np.array([[0,0,0]]), slice_size_lr, slice_size_lr, slice_size_lr), trainable=False), 0)\n",
    "process_indices_hr = tf.expand_dims(tf.Variable(process_indices(np.array([[0,0,0]]), slice_size_hr, slice_size_hr, slice_size_hr), trainable=False), 0)\n",
    "\n",
    "ind_lr = tf.placeholder(dtype=tf.int32, shape=(None, 3), name=\"ind_lr\")\n",
    "ind_hr = tf.placeholder(dtype=tf.int32, shape=(None, 3), name=\"ind_hr\")\n",
    "\n",
    "final_ind_lr = tf.reshape(tf.add(tf.expand_dims(ind_lr,1), process_indices_lr), (-1, 3), name=\"lr_ind_reshape\")\n",
    "final_ind_hr = tf.reshape(tf.add(tf.expand_dims(ind_hr,1), process_indices_hr), (-1, 3), name=\"hr_ind_reshape\")\n",
    "\n",
    "ϵ = tf.placeholder(dtype=tf.float32, shape=(None), name=\"epsilon\")\n",
    "ϵ_ = tf.expand_dims(tf.expand_dims(tf.expand_dims(tf.expand_dims(ϵ, 1), 1), 1), 1)\n",
    "λ = tf.placeholder(dtype=tf.float32, shape=(), name=\"lambda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to pass a cube to the graph, we will define two placeholders; one for passing low-resolution density field and another for the high-resolution initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_δ_lr = tf.placeholder(dtype=tf.float32, \n",
    "                          shape=(1, slice_size_lr, slice_size_lr, slice_size_lr, 1),\n",
    "                          name=\"single_delta_lr\")\n",
    "\n",
    "single_δ_ic = tf.placeholder(dtype=tf.float32, \n",
    "                          shape=(1, slice_size_hr, slice_size_hr, slice_size_hr, 1),\n",
    "                          name=\"single_delta_ic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we train on smaller boxes, we eventually verify the network's performance on an unseen simulation (*test set*), where we predict simulations larger than the ones used to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_size_lr = 128\n",
    "big_slice_size_lr = int(big_size_lr+2*pad)\n",
    "\n",
    "big_δ_lr = tf.placeholder(dtype=tf.float32, \n",
    "                       shape=(1, big_slice_size_lr, big_slice_size_lr, big_slice_size_lr, 1),\n",
    "                       name=\"big_delta_lr\")\n",
    "\n",
    "big_slice_size_ic = big_size_lr*2\n",
    "\n",
    "big_δ_ic = tf.placeholder(dtype=tf.float32, \n",
    "                       shape=(1, big_slice_size_ic, big_slice_size_ic, big_slice_size_ic, 1),\n",
    "                       name=\"big_delta_ic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low-resolution densities, and corresponding initial conditions, stored in the graph are then sliced using the fed indices and `m` batches are concatenated if we wanted to increase the batch size. These tensors are then rotated and/or mirrored (the same way for both the density and the initial conditions) as data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_lr = tf.gather_nd(δ_lr, final_ind_lr, name=\"gather_lr\")\n",
    "gather_ic = tf.gather_nd(δ_ic, final_ind_hr, name=\"gather_ic\")\n",
    "gather_hr = tf.gather_nd(δ_hr, final_ind_hr, name=\"gather_hr\")\n",
    "\n",
    "real_δ_lr = tf.reshape(gather_lr, (-1, slice_size_lr, slice_size_lr, slice_size_lr, 1), name=\"lr_reshape\")\n",
    "real_δ_ic = tf.reshape(gather_ic, (-1, slice_size_hr, slice_size_hr, slice_size_hr, 1), name=\"ic_reshape\")\n",
    "real_δ_hr = tf.reshape(gather_hr, (-1, slice_size_hr, slice_size_hr, slice_size_hr, 1), name=\"hr_reshape\")\n",
    "\n",
    "δ_lr_rotated = tf.squeeze(tf.map_fn(rotations, [tf.expand_dims(real_δ_lr, 1), rotation, inv], dtype=tf.float32), 1)\n",
    "δ_ic_rotated = tf.squeeze(tf.map_fn(rotations, [tf.expand_dims(real_δ_ic, 1), rotation, inv], dtype=tf.float32), 1)\n",
    "δ_hr_rotated = tf.squeeze(tf.map_fn(rotations, [tf.expand_dims(real_δ_hr, 1), rotation, inv], dtype=tf.float32), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the approximate Wasserstein distance\n",
    "\n",
    "The output of the critic is a single scalar which is used to compute the approximately learned Wasserstein distance between the predicted and true high-resolution density field given a particular generative network. This output can therefore be used to compute the loss function which is minimized to train the emulator.\n",
    "\n",
    "First we need the critic result of the real high-resolution density field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T11:54:50.480473Z",
     "start_time": "2018-06-13T11:54:44.635933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC\n",
      "Tensor(\"W/Slice:0\", shape=(?, 28, 28, 28, 1), dtype=float32)\n",
      "density\n",
      "Tensor(\"W/Slice_1:0\", shape=(?, 28, 28, 28, 1), dtype=float32)\n",
      "data\n",
      "Tensor(\"W/concat:0\", shape=(?, 28, 28, 28, 2), dtype=float32)\n",
      "Tensor(\"W/LeakyRelu:0\", shape=(?, 11, 11, 11, 8), dtype=float32)\n",
      "Tensor(\"W/LeakyRelu_1:0\", shape=(?, 7, 7, 7, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"W\") as scope:\n",
    "    W_real = tf.identity(W(δ_hr_rotated, δ_ic_rotated, pad_hr, slice_size_hr, reference=True), name=\"W_real\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we want the output of the emulator (and we will also create another input to the graph provided by the fed density). We also need to grab the result of the critic from this generated output from the emulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Squeeze:0\", shape=(?, 20, 20, 20, 1), dtype=float32)\n",
      "Tensor(\"G_com/up_sampling3d/concat_2:0\", shape=(?, 40, 40, 40, 1), dtype=float32)\n",
      "Tensor(\"G_com/LeakyRelu:0\", shape=(?, 34, 34, 34, 24), dtype=float32)\n",
      "Tensor(\"G_com/LeakyRelu_1:0\", shape=(?, 34, 34, 34, 48), dtype=float32)\n",
      "Tensor(\"G_com/concat_2:0\", shape=(?, 34, 34, 34, 72), dtype=float32)\n",
      "Tensor(\"G_com/Relu:0\", shape=(?, 28, 28, 28, 48), dtype=float32)\n",
      "Tensor(\"G_com/add_24:0\", shape=(?, 28, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"single_delta_lr:0\", shape=(1, 20, 20, 20, 1), dtype=float32)\n",
      "Tensor(\"G_com/up_sampling3d_1/concat_2:0\", shape=(1, 40, 40, 40, 1), dtype=float32)\n",
      "Tensor(\"G_com/LeakyRelu_2:0\", shape=(1, 34, 34, 34, 24), dtype=float32)\n",
      "Tensor(\"G_com/LeakyRelu_3:0\", shape=(1, 34, 34, 34, 48), dtype=float32)\n",
      "Tensor(\"G_com/concat_6:0\", shape=(1, 34, 34, 34, 72), dtype=float32)\n",
      "Tensor(\"G_com/Relu_1:0\", shape=(1, 28, 28, 28, 48), dtype=float32)\n",
      "Tensor(\"G_com/add_49:0\", shape=(1, 28, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"big_delta_lr:0\", shape=(1, 128, 128, 128, 1), dtype=float32)\n",
      "Tensor(\"G_com/up_sampling3d_2/concat_2:0\", shape=(1, 256, 256, 256, 1), dtype=float32)\n",
      "Tensor(\"G_com/LeakyRelu_4:0\", shape=(1, 250, 250, 250, 24), dtype=float32)\n",
      "Tensor(\"G_com/LeakyRelu_5:0\", shape=(1, 250, 250, 250, 48), dtype=float32)\n",
      "Tensor(\"G_com/concat_10:0\", shape=(1, 250, 250, 250, 72), dtype=float32)\n",
      "Tensor(\"G_com/Relu_2:0\", shape=(1, 244, 244, 244, 48), dtype=float32)\n",
      "Tensor(\"G_com/add_74:0\", shape=(1, 244, 244, 244, 1), dtype=float32)\n",
      "IC\n",
      "Tensor(\"W_1/Slice:0\", shape=(?, 28, 28, 28, 1), dtype=float32)\n",
      "data\n",
      "Tensor(\"W_1/concat:0\", shape=(?, 28, 28, 28, 2), dtype=float32)\n",
      "Tensor(\"W_1/LeakyRelu:0\", shape=(?, 11, 11, 11, 8), dtype=float32)\n",
      "Tensor(\"W_1/LeakyRelu_1:0\", shape=(?, 7, 7, 7, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"G_com\") as scope:\n",
    "    generated_δ_hr = tf.identity(G(δ_lr_rotated, δ_ic_rotated, num_convs), name=\"generated_delta_hr\")\n",
    "    scope.reuse_variables()\n",
    "    output = tf.identity(G(single_δ_lr, single_δ_ic, num_convs, slice_size_hr), name=\"output\")\n",
    "    scope.reuse_variables()\n",
    "    big_output = tf.identity(G(big_δ_lr, big_δ_ic, num_convs, big_slice_size_ic), name=\"big_output\")\n",
    "    \n",
    "with tf.variable_scope(\"W\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    W_gen = tf.identity(W(generated_δ_hr, δ_ic_rotated, pad_hr, slice_size_hr), name=\"W_gen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For improved training performance, we implement the gradient penalty method via the addition of a penalty term in the critic loss, as an alternative to the standard weight clipping, to enforce the Lipschitz-1 constraint on the critic. This is a requirement for computing the approximate Wasserstein distance. The Lipschitz constraint is enforced by penalizing the gradient norm for random samples $\\hat{\\mathbf{x}} \\sim \\mathbb{P}_{\\hat{\\mathbf{x}}}$, where $\\hat{\\mathbf{x}} = \\epsilon \\mathbf{x} + (1 - \\epsilon) \\tilde{\\mathbf{x}}$ and $\\epsilon$ is sampled randomly and uniformly, $\\epsilon \\in [0,1]$.\n",
    "\n",
    "We calculate this set of random samples, pass it through the critic and then calculate the outputs gradient with respect to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC\n",
      "Tensor(\"W_2/Slice:0\", shape=(?, 28, 28, 28, 1), dtype=float32)\n",
      "data\n",
      "Tensor(\"W_2/concat:0\", shape=(?, 28, 28, 28, ?), dtype=float32)\n",
      "Tensor(\"W_2/LeakyRelu:0\", shape=(?, 11, 11, 11, 8), dtype=float32)\n",
      "Tensor(\"W_2/LeakyRelu_1:0\", shape=(?, 7, 7, 7, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "δ_hr_rotated = tf.slice(δ_hr_rotated, [0, pad_hr, pad_hr, pad_hr, 0], [-1, slice_size_hr-2*pad_hr, slice_size_hr-2*pad_hr, slice_size_hr-2*pad_hr, -1])\n",
    "\n",
    "hat_δ_hr = ϵ_ * δ_hr_rotated + (1. - ϵ_) * generated_δ_hr\n",
    "\n",
    "with tf.variable_scope(\"W\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    W_hat = tf.identity(W(hat_δ_hr, δ_ic_rotated, pad_hr, slice_size_hr), name=\"W_hat\")\n",
    "    \n",
    "W_grad = tf.gradients(W_hat, hat_δ_hr, name=\"W_grad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function for the critic and the emulator can now be defined. We define a critic loss with the gradient penalty to be optimised, and a pure measure of the Wasserstein distance for tracking the distance. The emulator loss does not need the gradient penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_loss = W_gen - W_real\n",
    "W_loss_g = tf.reduce_mean(W_loss + λ * (tf.norm(W_grad) - 1.)**2.)\n",
    "W_loss_n = tf.identity(-tf.reduce_mean(W_loss), name=\"W_loss_n\")\n",
    "G_loss = tf.reduce_mean(- W_loss, name=\"G_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Adam optimizer to train the super-resolution emulator and critic networks\n",
    "Standard choice of hyperparameters for the Adam optimizer.\n",
    "\n",
    "Since we want to only update the weights of the critic for the critic update and only update the weights of the emulator for the emulator update, we cycle through the gradient calculations and remove the superfluous weight update operations from the list of operations to be given to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T11:54:55.006493Z",
     "start_time": "2018-06-13T11:54:53.346030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /soft/tensorflow-python/1.15.0-py3.7.3/site-package/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "α = 1e-4\n",
    "β1 = 0.5\n",
    "β2 = 0.999\n",
    "W_opt = tf.train.AdamOptimizer(α, β1, β2)\n",
    "W_grad = W_opt.compute_gradients(W_loss_g)\n",
    "W_grad_ = []\n",
    "for i in W_grad:\n",
    "    if \"W\" in i[1].name:\n",
    "        W_grad_.append(i)\n",
    "W_train = W_opt.apply_gradients(W_grad_, name=\"W_train\")\n",
    "        \n",
    "G_opt = tf.train.AdamOptimizer(α, β1, β2)\n",
    "G_grad = G_opt.compute_gradients(G_loss)\n",
    "G_grad_ = []\n",
    "for i in G_grad:\n",
    "    if \"G\" in i[1].name:\n",
    "        G_grad_.append(i)\n",
    "G_train = G_opt.apply_gradients(G_grad_, name=\"G_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a session to launch the graph \n",
    "\n",
    "We now feed in the low- and high-resolution density fields and initial conditions to be stored in the graph so that they do not have to be passed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T11:55:00.122196Z",
     "start_time": "2018-06-13T11:54:56.232564Z"
    }
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer(), \n",
    "         feed_dict={\"initialise_low_res_density:0\": lr_final,\n",
    "                    \"initialise_high_res_density:0\": hr_final,\n",
    "                    \"initialise_high_res_IC:0\": hr_initial})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training preparations\n",
    "We choose a standard value for the arbitrary penalty coefficient, which has been shown to work well for a range of network architectures and data sets, and update the weights of the critic five times per single emulator update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T11:55:02.093402Z",
     "start_time": "2018-06-13T11:55:02.085676Z"
    }
   },
   "outputs": [],
   "source": [
    "λ_value = 10.\n",
    "n_critic = 10\n",
    "epochs = 500000\n",
    "W_loss_training = []\n",
    "G_loss_training = []\n",
    "W_loss_validation = []\n",
    "G_loss_validation = []\n",
    "m = 1\n",
    "i_epoch = 1\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training routine\n",
    "The training proceeds for a set number of weight updates, until an overall convergence of the emulator is achieved. The training rationale is to reduce the Wasserstein distance between the true and generated high-resolution density fields, conditional on the initial conditions, such that the emulator learns the correct mapping from low- to high-resolution density fields.\n",
    "\n",
    "The training steps are as follows:\n",
    "- The input to the emulator is randomly chosen and the corresponding true high-resolution density volume is selected;\n",
    "- To encode some further symmetries through our training set, we also perform a rotation of the selected patches, thereby extracting the input 3D slice from a randomly oriented region, and/or randomly mirrored along the three axes;\n",
    "- The initial training step involves the optimization of the weights of the critic network to minimize the augmented loss function (including the gradient penalty), while concurrently freezing the parameters of the emulator;\n",
    "- The weights of the critic must be updated $n_{\\mathrm{critic}}$ times, where $n_\\mathrm{critic}$ is sufficient for the critic to converge;\n",
    "- In the subsequent step, the critic weights are temporarily anchored, and the emulator parameters are adjusted;\n",
    "- The emulator employs the gradient of the Wasserstein loss function w.r.t its parameters for training;\n",
    "- The training routine then proceeds in iterative fashion, until an overall convergence of the emulator is achieved.\n",
    "\n",
    "We use a $512^3$ simulation box for training, where we use a large portion of the box for training and the remaining section for validation, such that we utilize non-mutual parts of the box for the training and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tq = tqdm.trange(epochs, leave = True, desc = \"Epochs\")\n",
    "for e in tq:\n",
    "    for t in range(n_critic):\n",
    "        i = np.random.randint(0, 25, m)\n",
    "        z = np.random.randint(0, 2, m)\n",
    "        ind_value_1 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "        ind_value_2 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "        ind_value_3 = np.random.randint(pad, lr_resolution - slice_size_lr - (slice_size_lr - pad), m)\n",
    "        ϵ_value = np.random.uniform(0, 1, m)\n",
    "        sess.run(\"W_train\",\n",
    "                 feed_dict={\"ind_lr:0\": np.array([ind_value_1, ind_value_2, ind_value_3]).T,\n",
    "                            \"ind_hr:0\": np.array([2*ind_value_1, 2*ind_value_2, 2*ind_value_3]).T,\n",
    "                            \"epsilon:0\": ϵ_value,\n",
    "                            \"lambda:0\": λ_value,\n",
    "                            \"rotation:0\": i,\n",
    "                            \"inversion:0\": z})\n",
    "\n",
    "    # W training loss\n",
    "    i = np.random.randint(0, 25, m)\n",
    "    z = np.random.randint(0, 2, m)\n",
    "    ind_value_1 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "    ind_value_2 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "    ind_value_3 = np.random.randint(pad, lr_resolution - slice_size_lr - (slice_size_lr - pad), m)\n",
    "    ϵ_value = np.random.uniform(0, 1, m)\n",
    "    W_loss_training.append(sess.run(\"W_loss_n:0\", \n",
    "                                    feed_dict={\"ind_lr:0\": np.array([ind_value_1, ind_value_2, ind_value_3]).T,\n",
    "                                               \"ind_hr:0\": np.array([2*ind_value_1, 2*ind_value_2, 2*ind_value_3]).T,\n",
    "                                               \"epsilon:0\": ϵ_value,\n",
    "                                               \"lambda:0\": λ_value,\n",
    "                                               \"rotation:0\": i,\n",
    "                                               \"inversion:0\": z}))\n",
    "   \n",
    "    # W validation loss\n",
    "    i = np.random.randint(0, 25, m)\n",
    "    z = np.random.randint(0, 2, m)\n",
    "    ind_value_1 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "    ind_value_2 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "    ind_value_3 = np.array([lr_resolution - (slice_size_lr - pad) for i in range(m)])\n",
    "    ϵ_value = np.random.uniform(0, 1, m)\n",
    "    W_loss_validation.append(sess.run(\"W_loss_n:0\", \n",
    "                                      feed_dict={\"ind_lr:0\": np.array([ind_value_1, ind_value_2, ind_value_3]).T,\n",
    "                                                 \"ind_hr:0\": np.array([2*ind_value_1, 2*ind_value_2, 2*ind_value_3]).T,\n",
    "                                                 \"epsilon:0\": ϵ_value,\n",
    "                                                 \"lambda:0\": λ_value,\n",
    "                                                 \"rotation:0\": i,\n",
    "                                                 \"inversion:0\": z}))\n",
    "\n",
    "    # G training loss\n",
    "    i = np.random.randint(0, 25, m)\n",
    "    z = np.random.randint(0, 2, m)\n",
    "    ind_value_1 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "    ind_value_2 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "    ind_value_3 = np.random.randint(pad, lr_resolution - slice_size_lr - (slice_size_lr - pad), m)\n",
    "    _, G_loss_temp = sess.run([\"G_train\", \"G_loss:0\"],\n",
    "                              feed_dict={\"ind_lr:0\": np.array([ind_value_1, ind_value_2, ind_value_3]).T,\n",
    "                                         \"ind_hr:0\": np.array([2*ind_value_1, 2*ind_value_2, 2*ind_value_3]).T,\n",
    "                                         \"rotation:0\": i,\n",
    "                                         \"inversion:0\": z})\n",
    "    G_loss_training.append(G_loss_temp)\n",
    "   \n",
    "    # G validation loss\n",
    "    i = np.random.randint(0, 25, m)\n",
    "    z = np.random.randint(0, 2, m)\n",
    "    ind_value_1 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "    ind_value_2 = np.random.randint(pad, lr_resolution - (slice_size_lr - pad), m)\n",
    "    ind_value_3 = np.array([lr_resolution - (slice_size_lr - pad) for i in range(m)])\n",
    "    G_loss_validation.append(sess.run(\"G_loss:0\", \n",
    "                                      feed_dict={\"ind_lr:0\": np.array([ind_value_1, ind_value_2, ind_value_3]).T,\n",
    "                                                 \"ind_hr:0\": np.array([2*ind_value_1, 2*ind_value_2, 2*ind_value_3]).T, \n",
    "                                                 \"rotation:0\": i,\n",
    "                                                 \"inversion:0\": z}))\n",
    "    \n",
    "    if i_epoch%100000 == 0:\n",
    "        m *= 2\n",
    "    i_epoch += 1\n",
    "    \n",
    "    tq.set_postfix(W_training_loss = W_loss_training[-1], \n",
    "                   G_training_loss = G_loss_training[-1], \n",
    "                   W_validation_loss = W_loss_validation[-1], \n",
    "                   G_validation_loss = G_loss_validation[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"predictions/training_validation_loss.npz\", W_loss_training=W_loss_training, G_loss_training=G_loss_training, W_loss_validation=W_loss_validation, G_loss_validation=G_loss_validation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
